% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/preprocessPlp.R
\name{featureReducer}
\alias{featureReducer}
\title{featureReduction}
\usage{
featureReducer(plpData, analysisSelector = NULL, covariateSelector = NULL,
  wrapper = NULL, matrixFactor = F, outcomeId = 2, cohortId = NULL)
}
\arguments{
\item{plpData}{An object of type \code{plpData} - the patient level prediction
data extracted from the CDM.}

\item{modelSettings}{A list defining the model to train, the cohortId, the outcomeId,
                                        preprocessing options and model parameters.

                                        The potential models are:
                                        Logistic regression with lasso regularisation: 'lr-lasso'
                                        Random forest: 'rf'
                                        Gradient boosting machineL 'gbm'
                                        Logistic regression with elastic new regularisation: 'lr-enet'

                                        The potential preprocessing settings are:
                                        Wrapper feature selection using lasso logistic regression: 'lr-lasso'
                                        Selecting only the condition/drug era features: 'allEra'

                                        The parameter setting depend on the model-
                                        model='lr-lasso' has the parameter val specificying the initial variance
                                        model='rf' has the parameters ntree, max_depth and mtry
                                        model='gbm' has the parameters ntree, bal (class balance), nrowSample (fraction of training data people to use per tree),
                                               ncolSample (fraction of training data features to include into each tree)
                                        model='lr-enet' has the parameters alpha, ...

                                        example: list(model='gbm', cohortId=NULL, outcomeId=c(1,2),
                                                      preprocess='lr-lasso', param=list(ntree=50, bal=F, nrowsample=0.6))
                                        Would train a gradient boosting machine to predict the outcomes 1 and 2
                                        using the features selected by logistic regression with lasso regularisation
                                        with the model settings ntree 50, no class label balance and 0.6 training data rows
                                        used per tree.}
}
\value{
A list of class plpData containing the original plpData but with a reduced number of covariates and
details of the feature selection/engineering stored in the metaData
\describe{ \item{cohorts}{An ffdf object listing all persons and their prediction periods. This
object will have these fields: row_id (a unique ID per period), person_id, cohort_start_date,
cohort_id, time (number of days in the window).} \item{outcomes}{An ffdf object listing all
outcomes per period. This object will have these fields: row_id, outcome_id, outcome_count,
time_to_event.} \item{exclude}{Either NULL or an ffdf object listing per outcome ID which windows
had the outcome prior to the window. This object will have these fields: rowId, outcomeId.}
\item{covariates}{An ffdf object listing the baseline covariates per person in the cohorts. This is
done using a sparse representation: covariates with a value of 0 are omitted to save space. The
covariates object will have three columns: rowId, covariateId, and covariateValue. }
\item{covariateRef}{An ffdf object describing the covariates that have been extracted.}
\item{metaData}{A list of objects with information on how the plpData object was constructed
and feature reduction details.  The list member named 'featureReducer' contains the parameters
input into the featureReduction and the member named 'usedCovariateIds' contains the set of covariteIds included.} }
}
\description{
Applies a variety of feature selection/engineering techniques
}
\details{
Users can pick from selecting a type of feature, applying wrapper feature selection using logistic regression
with the lasso regularisation or using matrix factorisation.
}

